{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b64c32",
   "metadata": {},
   "source": [
    "### 3. <u>Routing</u>:\n",
    "#### --> What is Routing in LangGraph?\n",
    "\n",
    "Routing in LangGraph refers to the ability to conditionally determine which node to execute next based on the current state or the output of a node. This is typiacally implemented using:\n",
    "\n",
    "* *<u>add_conditional_edges</u>*: A method that maps a node's output (or a condition function's result) to different possible next nodes.\n",
    "\n",
    "* *<u>State*</u>: The workflow's state can store variables that influence routing decisions.\n",
    "\n",
    "* *<u>Condition Functions*</u>: Functions that evaluate the state are node output to decide the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a66b0",
   "metadata": {},
   "source": [
    "### Key Concepts:\n",
    "\n",
    "* *<u>Dynamic Flow*</u>: Unlike a linear sequence, routing lets the graph adapt to intermediate results.\n",
    "\n",
    "* *<u>Condition Logic*</u>: You define rules (Example: \"if this, go here; if that, go there.\").\n",
    "\n",
    "* *<u>Flexibility*</u>: Combines well with paralyzation or sequential chains for complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308d4650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?\\n', additional_kwargs={}, response_metadata={'model': 'gemma3:270m', 'created_at': '2025-10-01T03:00:02.1506078Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1756661100, 'load_duration': 1411210600, 'prompt_eval_count': 10, 'prompt_eval_duration': 143715200, 'eval_count': 11, 'eval_duration': 194177100, 'model_name': 'gemma3:270m'}, id='run--73368bcf-c06f-44ac-80a5-c39a0c047690-0', usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model='gemma3:270m')\n",
    "response = llm.invoke(\"Hello\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b39fa",
   "metadata": {},
   "source": [
    "**We implement Routing with the help of `pydantic` and `with_sturctured_output()` attribute/function of llm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6490fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal,TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "# Schema for structured output to use as routing logic\n",
    "class Route(BaseModel):\n",
    "    step:Literal[\"poem\",\"story\",\"joke\"] = Field(description=\"The next step in the routing process\")\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "router = llm.with_structured_output(Route)\n",
    "\n",
    "# state\n",
    "class State(TypedDict):\n",
    "    input:str\n",
    "    decision:str\n",
    "    output:str\n",
    "\n",
    "# nodes\n",
    "def llm_call_1(state:State):\n",
    "    '''writes a story'''\n",
    "    result = llm.invoke(state['input'])\n",
    "    return {'output':result.content}\n",
    "\n",
    "def llm_call_2(state:State):\n",
    "    '''writes a joke'''\n",
    "    print(\"llm call 2 is invoked\")\n",
    "    result = llm.invoke(state['input'])\n",
    "    return {'output':result.content}\n",
    "\n",
    "def llm_call_3(state:State):\n",
    "    '''writes a poem'''\n",
    "    print(\"llm call 3 is invoked\")\n",
    "    result = llm.invoke(state['input'])\n",
    "    return {'output':result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfa5f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_router(state:State):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\"Route the input to story, joke or poem based on the user's request\"),\n",
    "            HumanMessage(state['input'])\n",
    "        ]\n",
    "    )\n",
    "    return {'decision':decision.step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "817806af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition Function\n",
    "def route_decision(state:State):\n",
    "    if state['decision'] == 'story':\n",
    "        return 'llm_call_1'\n",
    "    elif state['decision'] == 'joke':\n",
    "        return 'llm_call_2'\n",
    "    elif state['decision'] == 'poem':\n",
    "        return 'llm_call_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "851695cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x23565736030>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the workflow\n",
    "from langgraph.graph import StateGraph,START, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"llm_call_1\",llm_call_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6836bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_router(state:State):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\"Route the input to story, joke or poem based on the user's request\"),\n",
    "            HumanMessage(state['input'])\n",
    "        ]\n",
    "    )\n",
    "    return {'decision':decision.step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f663b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition Function\n",
    "def route_decision(state:State):\n",
    "    if state['decision'] == 'story':\n",
    "        return 'llm_call_1'\n",
    "    elif state['decision'] == 'joke':\n",
    "        return 'llm_call_2'\n",
    "    elif state['decision'] == 'poem':\n",
    "        return 'llm_call_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0614278c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x23566e83e30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the workflow\n",
    "from langgraph.graph import StateGraph,START, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"llm_call_1\",llm_call_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20ee5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call_router(state:State):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\"Route the input to story, joke or poem based on the user's request\"),\n",
    "            HumanMessage(state['input'])\n",
    "        ]\n",
    "    )\n",
    "    return {'decision':decision.step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89984d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition Function\n",
    "def route_decision(state:State):\n",
    "    if state['decision'] == 'story':\n",
    "        return 'llm_call_1'\n",
    "    elif state['decision'] == 'joke':\n",
    "        return 'llm_call_2'\n",
    "    elif state['decision'] == 'poem':\n",
    "        return 'llm_call_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9602f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the workflow\n",
    "from langgraph.graph import StateGraph,START, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"llm_call_1\",llm_call_1)\n",
    "builder.add_node(\"llm_call_2\",llm_call_2)\n",
    "builder.add_node(\"llm_call_3\",llm_call_3)\n",
    "builder.add_node(\"llm_call_router\",llm_call_router)\n",
    "\n",
    "builder.add_edge(START,\"llm_call_router\")\n",
    "builder.add_conditional_edges(\n",
    "    \"llm_call_router\",\n",
    "    route_decision,\n",
    "    {\n",
    "        'llm_call_1':'llm_call_1',\n",
    "        'llm_call_2':'llm_call_2',\n",
    "        'llm_call_3':'llm_call_3',\n",
    "    },\n",
    ")\n",
    "builder.add_edge(\"llm_call_1\",END)\n",
    "builder.add_edge(\"llm_call_2\",END)\n",
    "builder.add_edge(\"llm_call_3\",END)\n",
    "\n",
    "builder_graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91d1c07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the agentic AI system get fired from its job? \n",
      "\n",
      "Because it kept asking for more!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = builder_graph.invoke({'input':'Write me a joke about agentic ai system'})\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297cdba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
